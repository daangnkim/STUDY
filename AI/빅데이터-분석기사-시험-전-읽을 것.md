## 파트1
## 파트2
sklearn.model_selection
sklearn.metrics
sklearn.preprocessing
sklearn.ensemble

### 1. 이진 분류
2개로 분류한다: 이직 성공 가능성, 이직 실패 가능성

from **sklearn.metrics** import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

### 2. 다중 분류
3개 이상으로 분류한다: 신용 점수가 매우 좋음, 보통, 나쁨

from **sklearn.metrics** import accuracy_score, precision_score, recall_score, f1_score
avarage="micro" | "macro" | "weighted"

### 3. 회귀 분류
from **sklearn.metrics** import mean_squared_error, mean_absolute_error, r2_score, mean_squared_error, mean_squared_log_error

mean_absolute_percentage_error = (abs((y_true - y_pred) / y_true)).mean() * 100 📌

score가 붙은 애들은 1에 가까울수록 좋고, error가 붙은 애들은 낮을수록 좋다. 📌

### 4. 튜닝
결측치 처리 -> 인코딩 -> 스케일링 순으로 진행한다.
```python
cols = train.select_dtypes(include=['int64', 'float64'])

from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
scaler = StandardScaler()
train[cols] = scaler.fit_transform(train[cols]) 📌
test[cols] = scaler.transform(test[cols]) 📌
```

(2) 회귀에서 스케일링 -> 인코딩
스케일링은 원래 숫자 변수들에만 적용한다. 인코딩해서 숫자가 된 변수들 말고!
RandomForestClassifier(max_depth, n_estimators)
이진 분류의 max_depth는 3 ~ 7, n_estimators는 200 ~ 500
다중 분류의 max_depth는 5 ~ 10, n_estimators는 200 ~ 500
회귀의 max_depth는 10 ~ 20, n_estimators는 200 ~ 500

### 5. 기타
#### 인코딩 관련
1. 라벨 인코딩
```python
from sklearn.preprocessing import LabelEncoder
for col in cols:
	le = LabelEncoder()
	train[col] = le.fit_transform(train[col])
	test[col] = le.transform(test[col])
```
1. 원핫 인코딩
	pd.get_dummies()
#### 검증 데이터 나누기
```python
from sklearn.model_selection import train_test_split
X_tr, X_val, y_tr, y_val = train_test_split(train, target, test_size=0.2, random_state=0)
```
#### 학습
1. 랜덤 포레스트 분류
```python
from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassfier(random_state=0)
rf.fit(X_tr, y_tr)
y_pred = rf.predict(X_val)
```

2. 랜덤 포레스트 회귀
```python
from sklearn.ensemble import RandomForestRegressor
rf = RandomForestRegressor(random_state=0)
rf.fit(X_tr, y_tr)
y_pred = rf.predict(X_val)
```

#### 제출
```python
pred = rf.predict_proba(test)
submit = pd.DataFrame({
	'pred': pred[:, 1] 📌 매우 중요
})
submit.to_csv("result.csv", index=False)
```
회귀에서 사람 수를 예측하는 경우등 이라면 정수로 반올림하고, 그렇지 않다면 그냥 제출한다.

## 파트 3

### 1. 검정
from scipy import stats
#### 단일 표본 검정
stats.ttest_1samp(a, b, alternative="")
stats.shaprio() - 정규성
stats.wilcoxon(a, b, alternative="")
#### 대응 표본 검정
stats.ttest_1samp(a, b, alternative="")
stats.shaprio(diff) - 정규성
stats.wilcoxon(a, b, alternative="")
#### 독립 표본 검정
stats.ttest_1samp(a, b, alternative="")
stats.shapiro() - 정규성
stats.levene(a, b) - 등분산성
stats.mannwhiteneyu(a, b, alternative="")

shapiro든 levene이든 0.05이상이면 정규성이나 등분산성을 만족한다.

### 2. 회귀 + 로지스틱 회귀
from statsmodels.formula.api from ols
from statsmodels.formula.api from logit
logit("종속변수 ~ 독립변수1 + 독립변수2 + O(독립변수3), data=df).fit()
오즈비를 구하는 경우 np.exp(df['몸무게'])

## 3. 기타
검정 통계량은 ttest_ind 혹은 ttest_rel을 수행하고 나온 statistics 값이다.

온도의 회귀계수가 유의한지 통계적으로 검증하시오.
model.pvalues['온도']

bmi 변수가 한 단위 증가할 때 질병 발생의 오즈비
model.params['bmi']
np.exp(model.params['bmi'])

각 변수별 95%의 신뢰 구간 구하기
model.conf_int(alpha=0.05)

광고비 45, 직원 수 22일 때 95%의 신뢰구간과 예측구간 구하기
pred = model.get_prediction(new_data)
result = pred.summary_frame(alpha=0.05)
result

일원분산분석은
from statsmodels.stats.anova import anova_lm


이원분산분석은
from statsmodels.api as sm
sm.stats.anova_lm(model)

일원분산분석은 하나의 범주형 변수가 하나의 연속형 변수에 미치는 영향 분석
이원분산분석은 두 개의 범주형 독리 변수가 하나의 ~ 
단순회귀분석은 두 변수간의 관계
다중회귀분석은 세 변수간의 관계
로지스틱회귀는 독립변수 1개 이상이고 범주형 종속변수일때

## 궁금증
- 숫자 결측치 처리는 어떻게함?

## 오늘 가기전 해야할 일
- [ ] 작업형1 - 파트 14번부터 35번까지 풀기
	- [ ] 28번
	- [ ] 33번
	- [ ] 35번
	- [ ] 36번
	- [ ] 37번
	- [ ] 38번
	- [ ] 39번
- [ ] 작업형2 - 다중분류 연습문제 2개 읽기
- [ ] 숫자 결측치 처리 하는 법 알아보기
- [x] 작업형3 - t검정 연습문제 풀기
- [ ] 작업형3 - 회귀쪽 연습문제 풀기
- [ ] 작업형3 - 일원 분산 분석, 이원 분산 분석