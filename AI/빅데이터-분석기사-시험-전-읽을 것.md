## íŒŒíŠ¸1
## íŒŒíŠ¸2
sklearn.model_selection
sklearn.metrics
sklearn.preprocessing
sklearn.ensemble

### 1. ì´ì§„ ë¶„ë¥˜
2ê°œë¡œ ë¶„ë¥˜í•œë‹¤: ì´ì§ ì„±ê³µ ê°€ëŠ¥ì„±, ì´ì§ ì‹¤íŒ¨ ê°€ëŠ¥ì„±

from **sklearn.metrics** import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

### 2. ë‹¤ì¤‘ ë¶„ë¥˜
3ê°œ ì´ìƒìœ¼ë¡œ ë¶„ë¥˜í•œë‹¤: ì‹ ìš© ì ìˆ˜ê°€ ë§¤ìš° ì¢‹ìŒ, ë³´í†µ, ë‚˜ì¨

from **sklearn.metrics** import accuracy_score, precision_score, recall_score, f1_score
avarage="micro" | "macro" | "weighted"

### 3. íšŒê·€ ë¶„ë¥˜
from **sklearn.metrics** import mean_squared_error, mean_absolute_error, r2_score, mean_squared_error, mean_squared_log_error
r2_scoreëŠ” 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¢‹ê³ , ì˜¤ì°¨ëŠ” ì‘ì„ìˆ˜ë¡ ì¢‹ë‹¤  ğŸ“Œ
mean_absolute_percentage_error = (abs((y_true - y_pred) / y_true)).mean() * 100 ğŸ“Œ

### 4. íŠœë‹
ê²°ì¸¡ì¹˜ ì²˜ë¦¬ -> ì¸ì½”ë”© -> ìŠ¤ì¼€ì¼ë§ ìˆœìœ¼ë¡œ ì§„í–‰í•œë‹¤.
```python
cols = train.select_dtypes(include=['int64', 'float64'])

from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
scaler = StandardScaler()
train[cols] = scaler.fit_transform(train[cols]) ğŸ“Œ
test[cols] = scaler.transform(test[cols]) ğŸ“Œ
```

(2) íšŒê·€ì—ì„œ ìŠ¤ì¼€ì¼ë§ -> ì¸ì½”ë”©
ìŠ¤ì¼€ì¼ë§ì€ ì›ë˜ ìˆ«ì ë³€ìˆ˜ë“¤ì—ë§Œ ì ìš©í•œë‹¤. ì¸ì½”ë”©í•´ì„œ ìˆ«ìê°€ ëœ ë³€ìˆ˜ë“¤ ë§ê³ !
RandomForestClassifier(max_depth, n_estimators)
ì´ì§„ ë¶„ë¥˜ì˜ max_depthëŠ” 3 ~ 7, n_estimatorsëŠ” 200 ~ 500
ë‹¤ì¤‘ ë¶„ë¥˜ì˜ max_depthëŠ” 5 ~ 10, n_estimatorsëŠ” 200 ~ 500
íšŒê·€ì˜ max_depthëŠ” 10 ~ 20, n_estimatorsëŠ” 200 ~ 500

### 5. ê¸°íƒ€
#### ì¸ì½”ë”© ê´€ë ¨
1. ë¼ë²¨ ì¸ì½”ë”©
```python
from sklearn.preprocessing import LabelEncoder
for col in cols:
	le = LabelEncoder()
	train[col] = le.fit_transform(train[col])
	test[col] = le.transform(test[col])
```
1. ì›í•« ì¸ì½”ë”©
	pd.get_dummies()
#### ê²€ì¦ ë°ì´í„° ë‚˜ëˆ„ê¸°
```python
from sklearn.model_selection import train_test_split
X_tr, X_val, y_tr, y_val = train_test_split(train, target, test_size=0.2, random_state=0)
```
#### í•™ìŠµ
1. ëœë¤ í¬ë ˆìŠ¤íŠ¸ ë¶„ë¥˜
```python
from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassfier(random_state=0)
rf.fit(X_tr, y_tr)
y_pred = rf.predict(X_val)
```

2. ëœë¤ í¬ë ˆìŠ¤íŠ¸ íšŒê·€
```python
from sklearn.ensemble import RandomForestRegressor
rf = RandomForestRegressor(random_state=0)
rf.fit(X_tr, y_tr)
y_pred = rf.predict(X_val)
```

#### ì œì¶œ
```python
pred = rf.predict_proba(test)
submit = pd.DataFrame({
	'pred': pred[:, 1] 
})
submit.to_csv("result.csv", index=False)
```
íšŒê·€ì—ì„œ ì‚¬ëŒ ìˆ˜ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê²½ìš°ë“± ì´ë¼ë©´ ì •ìˆ˜ë¡œ ë°˜ì˜¬ë¦¼í•˜ê³ , ê·¸ë ‡ì§€ ì•Šë‹¤ë©´ ê·¸ëƒ¥ ì œì¶œí•œë‹¤.

## íŒŒíŠ¸ 3

### 1. ê²€ì •
from scipy import stats
#### ë‹¨ì¼ í‘œë³¸ ê²€ì •
stats.ttest_1samp(a, b, alternative="")
stats.shaprio() - ì •ê·œì„±
stats.wilcoxon(a, b, alternative="")
#### ëŒ€ì‘ í‘œë³¸ ê²€ì •
stats.ttest_1samp(a, b, alternative="")
stats.shaprio(diff) - ì •ê·œì„±
stats.wilcoxon(a, b, alternative="")
#### ë…ë¦½ í‘œë³¸ ê²€ì •
stats.ttest_1samp(a, b, alternative="")
stats.shapiro() - ì •ê·œì„±
stats.levene(a, b) - ë“±ë¶„ì‚°ì„±
stats.mannwhiteneyu(a, b, alternative="")

### 2. íšŒê·€ + ë¡œì§€ìŠ¤í‹± íšŒê·€
from statsmodels.formula.api from ols
from statsmodels.formula.api from logit
logit("ì¢…ì†ë³€ìˆ˜ ~ ë…ë¦½ë³€ìˆ˜1 + ë…ë¦½ë³€ìˆ˜2 + O(ë…ë¦½ë³€ìˆ˜3), data=df).fit()
ì˜¤ì¦ˆë¹„ë¥¼ êµ¬í•˜ëŠ” ê²½ìš° np.exp(df['ëª¸ë¬´ê²Œ'])

## ê¶ê¸ˆì¦
- ìˆ«ì ê²°ì¸¡ì¹˜ ì²˜ë¦¬ëŠ” ì–´ë–»ê²Œí•¨?

## ì˜¤ëŠ˜ ê°€ê¸°ì „ í•´ì•¼í•  ì¼
- [ ] ì‘ì—…í˜•1 - íŒŒíŠ¸ 14ë²ˆë¶€í„° 35ë²ˆê¹Œì§€ í’€ê¸°
	- [ ] 28ë²ˆ
	- [ ] 33ë²ˆ
	- [ ] 35ë²ˆ
- [ ] ì‘ì—…í˜•2 - ë‹¤ì¤‘ë¶„ë¥˜ ì—°ìŠµë¬¸ì œ 2ê°œ ì½ê¸°
- [ ] ìˆ«ì ê²°ì¸¡ì¹˜ ì²˜ë¦¬ í•˜ëŠ” ë²• ì•Œì•„ë³´ê¸°
- [ ] ì‘ì—…í˜•3 - tê²€ì • ì—°ìŠµë¬¸ì œ í’€ê¸°
- [ ] ì‘ì—…í˜•3 - íšŒê·€ìª½ ì—°ìŠµë¬¸ì œ í’€ê¸°